---
layout: post
title: Robust High-Resolution Video Matting with Temporal Guidance
categories: paperReading
tags: [paper22, matting]
---
### Abstract

这篇是来自字节跳动的文章，提出了一种鲁棒的、实时的、支持高分辨率的视频人像扣图算法，在1080Ti上就能实现4K/76FPS以及HD/104FPS的高速处理能力。创新点在于，作者认为先前的扣图论文（这篇论文发表于2021年）将视频扣图转化为一帧一帧扣图，作者则是使用了循环结构去探索视频之间的时间关系，在时间连续性（*这个如何理解？*）和扣图质量上提升极大。

并且作者提出了一种新的训练策略，将模型在扣图任务和细分目标任务（目标分割？）（segmentation objective）同时训练，因此提升了模型的鲁棒性。

作者的方法并不使用辅助输入（例如三元图或者预先捕捉好的背景图），因此可以直接应用到现存的任务扣图应用上。

### Intro

扣图是一种从输入图像中预测aplha遮罩以及前景颜色的过程，通常来说，一张图片可以认为是前景 $F$ 和背景 $B$ 通过 $\alpha$ 的线性组合，即： 

 $ I=\alpha F + (1-\alpha)B $

从公式中可以看出，只要求出 $\alpha$ 和 $F$ 就能将前景融合（composite）到新的背景当中，达到背景替换的目的。

背景替换有很多的实际应用场景，比如视频会议、娱乐视频等等，绝大多数先前的方法（尽管是专门为视频设计），都是将视频帧当做独立的图像来处理，这些方法忽略了视频中最重要的因素：时间关系。

#### 时间关系

时间关系可以提升很多视频相关任务的性能，不限于视频扣图任务，主要原因几点：

1. 时间关系可以让模型预测到（应该是学习到）更多连续的结果（coherent），因为模型可以同时看到许多连续的帧，这显著的降低了闪烁（flicker），提升预测质量。
2. 利用时间关系（也可以理解为上下文关系）可以很好的提升模型鲁棒性，因为单个帧可能是信息模糊的。
3. 时间关系可以让模型学习到更多的有这时间关系的背景信息，这一点其实很重要：当相机移动时，由于视角（perspective）变化，主体背后的背景会显露出来。 即使相机是固定的，被遮挡（occluded）的背景仍然经常由于拍摄对象的移动而显露（reveal）出来。

因此作者使用了循环结构来利用（exploit）视频中的时间信息。并且作者提出了一种新的训练策略，即强制让模型在扣图和语义物体分割任务上同时训练，这样做的初衷是先前的方法往往在语义扣图数据集上进行训练，这样往往看起来很假，并且不能很好的在真是图片上得到应用。有一些方法在已经训练好的语义分割任务模型上进行热启动，但这样会导致在扣图训练过程中过拟合。也有一些任务使用对抗训练或者半监督训练当做额外的使用步骤。作者的方法因为同时训练扣图和语义分割任务，因此不需要上述额外的步骤。

### Related Work

#### trimap-based matting

传统的算法（非学习的，应该是机器学习？）需要人工标注好的三元图来解决三元图中的未知区域。这种方法是物体不可知的（agnostic），并不局限于人像扣图。这种方法经常应用与图片编辑软件，用户可以手动选择扣图区域并提供人工指引（美图秀秀？）。DVM这个方法仅需要视频的第一帧的三元图并且可以传递到视频的其余部分。

#### Background-based matting

这种方法需要使用额外的预先提取好的背景图像作为输入，该信息作为前景选择的隐含方式并提高了抠图精度（BGMv2），但是这种方法不能处理动态背景以及大范围的摄像头移动。

#### Segmentation

语义分割任务是需要预测每一个像素的类别，这种方法不需要额外的辅助输入。它的线性分割遮罩可以用来定位人像物体，但是如果直接应用于背景替换会导致强烈的假象（artifact），尽管如此分割任务在不需要额外的辅助输入上和扣图任务十分相似，并且作者的模型也是受到分割任务的启发而设计的。DeepLabV3模型提出的ASPP（Atrous Spatial Pyramid Pooling，空间金字塔池化）模块，并且在编码器中使用了空洞卷积来提升性能，这个设计在很多模型中得到应用，比如MobileNetV3，不过其将ASPP简化成了LR-ASPP。

#### Auxiliary-free matting

作者同样学习了无辅助扣图的方法，MODNet是最新的肖像（portrait）扣图方法，不过作者的方法可以在扣人物的全身像。

#### Video matting

极少有扣图网络（2021年）专门为视频设计，MODNet提出了一个技巧（trick），即比较相邻两帧之间的预测结果来减少闪烁，但是这样的方法并不能用于快速移动的物体，并且该模型同样是将视频帧当做独立的图像来处理的。作者使用了循环结构来利用视频时间信息。

#### High-resolution matting

基于块的微调被PointRend用于分割、被BGMv2用于扣图，这个方法仅仅是在选择好的块上使用卷积操作。另外的方法是使用引导过滤（Guided Filter），这是一种后处理滤波器，它在以高分辨率帧为指导的情况下联合对低分辨率预测进行上采样。深度引导滤波器（DGF）作为一个可学习的模块，可以在没有手动超参数的情况下使用网络端到端训练。 尽管基于过滤器的上采样功能不那么强大，但它速度更快并且得到所有推理框架的良好支持。

### Model Architecture

模型由一个提取单个视频帧新的的编码器、一个聚合时间信息的循环解码器以及用于高分辨率上采样的深度引导滤波器（DGF）组成。

#### Feature-Extraction Encoder

模型编码器的设计和主流语义分割网络的结构相似，因为这些网络具有较好的人物定位功能，而这个功能在扣图中是基本需求。作者这里采用的是MobileNetV3-Large作为骨干网络，该网络的最后一层使用了没有下采样的空洞卷积。编码器用于处理单个帧并在 $\frac{1}{2}$、$\frac{1}{4}$、 $\frac{1}{8}$ 以及 $\frac{1}{16}$ 范围上提取特征值，目的是为了后续循环解码器做准备。

#### Recurrent Decoder

作者没有使用注意力机制或是将多帧作为前馈传播的补充输入主要因为循环结构可以学习到连续视频流中什么信息改被遗忘、什么信息该被记住（遗忘门等），而其他两种方法依赖于固定的规则在每个间隙（interval）中移出旧信息并在有限的记忆池中插入新的信息（占内存？）。循环结构这种同时保留长短时间信息的能力更适合于抠图任务（感觉这里主要目的是交互视频中的时间信息，且要保证效率和质量，感觉这里可以做一些文章的）。

作者采用了ConvGRU在多范围上聚合时间信息（也就是编码器传来的多个比例信息）。ConvGRU因为比LSTM少了一些门所以参数有效（parameter efficience），这部分也需要专门写一篇文章学习了。

![截屏2022-06-15 上午10.07.08](/assets/images/md_image/RobustMatting/model.png)

可以从图中看出，解码器主要包括BottleNeck Block、上采样模块以及输出模块。

##### BottleNeck Block

该模块主要接在编码器的最后，在LR-ASPP后降低一半的通道数，目的是减少参数量。

##### Upsampling Block

上采样模块主要是和之前的编码器反着来，在多个范围上进行操作。它会聚合上一层传来的线性上采样输入、编码器传来的对应范围的特征图以及使用2 $\times$ 2大小的重复卷积层下采样输入图像得到的信息。之后会使用卷积+BN+ReLU来进行特征融合以及降维操作。最后就是使用GRU来分离和聚合（也就是最后聚合一次信息）。

##### Output block

输出层没有使用GRU的原因它是扩张的（expansive？没太理解这个意思）并且在这一层并不是很有效，因此就使用了传统的卷积操作。

****

作者发现在一半通道上（一般使用将为操作，作者写的是分离和聚合操作）上使用ConvGRU是高效且有效的（effective and effience）。这种设计有助于 ConvGRU 专注于聚合时间信息，而另一个拆分分支则转发特定于当前帧的空间特征。除了最后一个卷积使用了1 $\times$ 1的卷积核，其余的卷积都是3 $\times$ 3。

****

#### Deep Guided Filter Module

当处理4K或者HD视频的时候，首先将它按照比例 $s$ 进行下采样后再传入上述网络中，在得到低分辨率的alpha值、前景、隐藏特征后，将他们与高分辨率的输入帧同时输入到DGF中后得到高分辨率的alpha遮罩和前景。

